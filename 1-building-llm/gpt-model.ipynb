{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together `text-embedding.ipynb` and `attention.ipynb` into transformer blocks\n",
    "# define our parameters \n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define a placeholder class for the gpt model -- need to implement the transformer block and layer normalization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]) # text embeddings\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "# example of input data for the gpt model\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# initialize the class we created and pass in the example input\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape) # output is batch size * tokens * 50257 dimensions (vocab size)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# next: implement layer normalization. here is an example\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# create 2 training examples with 5 dimensions (features) each\n",
    "batch_example = torch.randn(2, 5) \n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU()) # activation function, turns outputs into only positive\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print initial mean and variance\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# apply layer normalization by subtracting the mean and dividing by standard deviation\n",
    "# now have 0 mean and 1 variance\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into a class\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5 # add to variance to prevent dividing by 0\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) # trainable parameters\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps) # normalize input\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# apply module to the sample batch\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next implement a feed forward network with GELU activation\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (y, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m([y_gelu, y_relu], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGELU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, i)\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m activation function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/llm/venv/lib/python3.11/site-packages/matplotlib/pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm/venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/llm/venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm/venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:475\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    472\u001b[0m         kw[prop_name] \u001b[38;5;241m=\u001b[39m val\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xy) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 475\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43m_check_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     y \u001b[38;5;241m=\u001b[39m _check_1d(xy[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/llm/venv/lib/python3.11/site-packages/matplotlib/cbook.py:1404\u001b[0m, in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert scalars to 1D arrays; pass-through arrays as is.\"\"\"\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;66;03m# Unpack in case of e.g. Pandas or xarray object\u001b[39;00m\n\u001b[0;32m-> 1404\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43m_unpack_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;66;03m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;66;03m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# Note this will strip unit information.\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m         \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m~/llm/venv/lib/python3.11/site-packages/matplotlib/cbook.py:2395\u001b[0m, in \u001b[0;36m_unpack_to_numpy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m xtmp\n\u001b[1;32m   2394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_array(x) \u001b[38;5;129;01mor\u001b[39;00m _is_jax_array(x):\n\u001b[0;32m-> 2395\u001b[0m     xtmp \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__array__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2397\u001b[0m     \u001b[38;5;66;03m# In case __array__() method does not return a numpy array in future\u001b[39;00m\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(xtmp, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[0;32m~/llm/venv/lib/python3.11/site-packages/torch/_tensor.py:1083\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
